# Import

Documentation: [import](https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html#_literal_sqoop_import_literal)

The import tool imports an individual table from an RDBMS to HDFS. Each row from a table is represented as a separate record in HDFS. Records can be stored as text files (one record per line), or in binary representation as Avro or SequenceFiles.

## Syntax

```bash
sqoop import (generic-args) (import-args)
sqoop-import (generic-args) (import-args)
```

While the Hadoop generic arguments must precede any import arguments, you can type the import arguments in any order with respect to one another.

### Common arguments

| Argument | Description |
|---|---|
| `--connect <jdbc-uri>` | Specify JDBC connect string |
| `--connection-manager <class-name>` | Specify connection manager class to use |
| `--driver <class-name>` | Manually specify JDBC driver class to use |
| `--hadoop-mapred-home <dir>` | Override $HADOOP_MAPRED_HOME |
| `--help` | Print usage instructions |
| `--password-file` | Set path for a file containing the authentication password |
| `-P` | Read password from console |
| `--password <password>` | Set authentication password |
| `--username <username>`	 | et authentication username |
| `--verbose` | Print more information while working |
| `--connection-param-file <filename>` | Optional properties file that provides connection parameters |
| `--relaxed-isolation` | Set connection transaction isolation to read uncommitted for the mappers. |

### Importing Data into HDFS

Use the full hostname or IP address of the database host that can be seen by all your remote nodes.  
**Do not** use **localhost**.

#### Create a password file

```bash
echo -n "1234" > password
hdfs dfs -copyFromLocal password /user/$USER/password
```

#### Command

```bash
sqoop import \
--driver com.mysql.jdbc.Driver \
--connect jdbc:mysql://192.168.**.**/menagerie \
--username master \
--password-file /user/$USER/password \
--table pet \
--target-dir /user/$USER/pet \
--delete-target-dir \
--direct \
--verbose
```

#### Check a result file

```bash
hdfs dfs -ls /user/$USER/pet

/user/$USER/pet/_SUCCESS
/user/$USER/pet/part-m-00000
```

```bash
hdfs dfs -cat /user/$USER/pet/part-m-00000
```

```csv
Fluffy,Harold,cat,f,1993-02-04,NULL
Claws,Gwen,cat,m,1994-03-17,NULL
Buffy,Harold,dog,f,1989-05-13,NULL
Fang,Benny,dog,m,1990-08-27,NULL
Bowser,Diane,dog,m,1979-08-31,1995-07-29
Chirpy,Gwen,bird,f,1998-09-11,NULL
Whistler,Gwen,bird,NULL,1997-12-09,NULL
Slim,Benny,snake,m,1996-04-29,NULL
```

### Importing Data into Hive

Set `HIVE_HOME`, `HIVE_CONF_DIR`, `HADOOP_CLASSPATH`.

```bash
echo 'export HIVE_HOME=/home/$USER/hive' >> ~/.bash_profile
echo 'export HIVE_CONF_DIR=$HIVE_HOME/conf' >> ~/.bash_profile
echo 'export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*' >> ~/.bash_profile
source ~/.bash_profile
```

#### Command

```bash
sqoop import \
... \
--hive-import \
--null-string '\\N' \
--null-non-string '\\N'
```

```bash
sqoop import \
--connect jdbc:mysql://localhost/menagerie \
--username master \
--password-file /user/vagrant/password \
--table pet \
-m 1 \
--direct \
--hive-import \
--null-string '\\N' \
--null-non-string '\\N' \
--verbose
```

Complete:

```bash
...

OK
20/03/17 08:52:24 INFO ql.Driver: OK
Time taken: 1.832 seconds
20/03/17 08:52:24 INFO CliDriver: Time taken: 1.832 seconds
...
20/03/17 08:52:24 INFO hive.HiveImport: Hive import complete.
20/03/17 08:52:24 INFO hive.HiveImport: Export directory is contains the _SUCCESS file only, removing the directory.
```

#### Check on HDFS

```bash
hdfs dfs -cat /user/hive/warehouse/pet/part-m-00000
```

```bash
FluffyHaroldcatf1993-02-04NULL
ClawsGwencatm1994-03-17NULL
BuffyHarolddogf1989-05-13NULL
FangBennydogm1990-08-27NULL
BowserDianedogm1979-08-311995-07-29
ChirpyGwenbirdf1998-09-11NULL
WhistlerGwenbirdNULL1997-12-09NULL
SlimBennysnakem1996-04-29NULL
```

#### Check on Hive

```bash
hive
```

```bash
hive> show tables;

OK
pet
Time taken: 4.231 seconds, Fetched: 1 row(s)
```

```bash
SELECT * FROM pet;

OK
Fluffy  Harold  cat     f       1993-02-04      NULL
Claws   Gwen    cat     m       1994-03-17      NULL
Buffy   Harold  dog     f       1989-05-13      NULL
Fang    Benny   dog     m       1990-08-27      NULL
Bowser  Diane   dog     m       1979-08-31      1995-07-29
Chirpy  Gwen    bird    f       1998-09-11      NULL
Whistler        Gwen    bird    NULL    1997-12-09      NULL
Slim    Benny   snake   m       1996-04-29      NULL
Time taken: 1.457 seconds, Fetched: 8 row(s)
```

#### Error

##### Permission

`ERROR Could not register mbeans java.security.AccessControlException: access denied ("javax.management.MBeanTrustPermission" "register")`

```bash
sudo vi $JAVA_HOME/lib/security/java.policy
```

Add this line to `grant` section.

```bash
permission javax.management.MBeanTrustPermission "register";
```

##### jackson.jar

`ERROR exec.DDLTask: java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.ObjectMapper.readerFor(Ljava/lang/Class;)Lcom/fasterxml/jackson/databind/ObjectReader;`

```bash
ls -al $SQOOP_HOME/lib | grep jackson
```

```bash
cd $SQOOP_HOME/lib
curl -O https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.3/jackson-core-2.10.3.jar;
curl -O https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.10.3/jackson-databind-2.10.3.jar;
curl -O https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.10.3/jackson-annotations-2.10.3.jar;
curl -O https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar;
curl -O https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar;
```
